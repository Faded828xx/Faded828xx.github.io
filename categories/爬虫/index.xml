<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>爬虫 on Faded828xx Blog</title>
    <link>https://faded828xx.github.io/categories/%E7%88%AC%E8%99%AB/</link>
    <description>Recent content in 爬虫 on Faded828xx Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 13 May 2021 22:10:36 +0800</lastBuildDate><atom:link href="https://faded828xx.github.io/categories/%E7%88%AC%E8%99%AB/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>豆瓣scrapy</title>
      <link>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3scrapy/</link>
      <pubDate>Thu, 13 May 2021 22:10:36 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3scrapy/</guid>
      <description>用scrapy实现先前的豆瓣爬虫，这里爬了top250，但是结果不是预期的顺序。
import scrapy from scrapy_demo.items import MovieItem class DoubanSpider(scrapy.Spider): name = &#39;douban&#39; allowed_domains = [&#39;douban.com&#39;] start_urls = [ &#39;https://movie.douban.com/top250&#39;, ] def parse(self, response): for i in range(0, 9): url = response.urljoin(&#39;?start=&#39; + str(25 * i)) yield scrapy.Request(url, callback=self.parse_dir_contents) def parse_dir_contents(self, response): # file = &#39;movie.html&#39; # 新建html页面并将爬取到的页面写入其中 # open(file, &#39;w&#39;).write(str(response.body.decode(&#39;utf-8&#39;))) # html = str(response.body.decode(&#39;utf-8&#39;)) # bs = bs4.BeautifulSoup(html, &#39;html.parser&#39;) # for movie in bs.find_all(&#39;div&#39;, class_=&#39;item&#39;): # item = MovieItem() # movie = str(movie) # 每部电影信息 # movielink = re.</description>
    </item>
    
    <item>
      <title>豆瓣小爬虫</title>
      <link>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3%E5%B0%8F%E7%88%AC%E8%99%AB/</link>
      <pubDate>Sun, 18 Apr 2021 14:29:53 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3%E5%B0%8F%E7%88%AC%E8%99%AB/</guid>
      <description>urllib爬取豆瓣网站
bs4解析html页面，获取有用信息并封装到数组中
sqlite3数据库，将数组存入其中
flask框架作为服务器，路由url地址到指定html页面，并从数据库取出变量动态写入页面
import re import sqlite3 import urllib.request import bs4 from flask import Flask, render_template import ssl ssl._create_default_https_context = ssl.SSLContext app = Flask(__name__) @app.route(&#39;/movie&#39;) def movie(): movies = saveDB() return render_template(&#39;movie.html&#39;, movies=movies) if __name__ == &#39;__main__&#39;: app.run() def getData(): # 返回25部电影数组 html = getHtml(&#39;https://movie.douban.com/top250&#39;) bs = bs4.BeautifulSoup(html, &#39;html.parser&#39;) datalist = [] # 存储25部电影 for item in bs.find_all(&#39;div&#39;, class_=&#39;item&#39;): data = [] item = str(item) # 每部电影信息 movielink = re.</description>
    </item>
    
  </channel>
</rss>
