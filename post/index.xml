<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Faded828xx Blog</title>
    <link>https://faded828xx.github.io/post/</link>
    <description>Recent content in Posts on Faded828xx Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 09 Aug 2021 14:42:04 +0800</lastBuildDate><atom:link href="https://faded828xx.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cookie和session的区别</title>
      <link>https://faded828xx.github.io/post/cookie%E5%92%8Csession%E7%9A%84%E5%8C%BA%E5%88%AB/</link>
      <pubDate>Mon, 09 Aug 2021 14:42:04 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/cookie%E5%92%8Csession%E7%9A%84%E5%8C%BA%E5%88%AB/</guid>
      <description>最近在看shiro的认证授权，了解一下登陆方式。
这里讲最基础的cookie和session。
简单的概念就不再说了。
session往往是服务器保存一个会话周期内的数据，比如验证码的流程：服务器生成一个验证码，将其保存到session中， 并将sessionID写入到响应体，客户端再发送请求时，附上sessionID和validateCode，服务器从对应的session中取出正确的验证码进行比较。如果没有session，很显然服务器不能将已发送出去的验证码和对应客户端一一匹配。而sessionID是服务器写入到cookie里的，客户端将cookie一起发送过去。cookie作为两者传输session的媒介，也有url的方式。
rememberMe功能是为客户端写入另一个cookie，可以设置expire时间，在有效期内，客户端发送请求会附加这段cookie，例如登陆用户名密码等，相当于客户端保存了你的登陆信息。
显而易见，session是服务器保存数据， 而cookie是客户端保存数据。
拿登陆来举例子，session可以保证你在一个会话周期内不需要多次输入用户名密码，而cookie的这个时间为cookie的有效期。
在一个会话周期内，服务器会将验证通过的用户信息保存到本地session中，然后对同一sessionID的请求，到对应的session中取出用户信息。如果服务器有多个节点，而彼此的session没有同步的话，可能会出现A服务器设置的session通过登陆后，再访问到B服务器时，发现该服务器上没有保存对应的Asession，因此在B服务器上需要重新认证。
session是简化了服务器的认证过程，防止对一个会话周期内的同一客户端进行多次认证。而cookie是简化了客户端的认证过程，避免用户再敲一遍登陆信息，实际上中间也有一次认证，服务器将cookie解析成登陆信息完成认证。
而Token是，服务器在认证后，拿到用户信息并生成Token（里面包含用户信息），然后将它发送给客户端，客户端每次请求时会带上这个Token。相当于 从session中 将用户信息保存到本地 然后根据请求再找数据；变成了 将这个信息直接交给客户端来保存， 然后服务器只需要解析这个Token就能直接拿到对应的用户信息了，当然中间会有服务器生成的签名来防止Token被伪造。存储用户信息的主体由服务器变成了客户端。</description>
    </item>
    
    <item>
      <title>面试一</title>
      <link>https://faded828xx.github.io/post/%E9%9D%A2%E8%AF%95%E4%B8%80/</link>
      <pubDate>Fri, 30 Jul 2021 13:34:05 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E9%9D%A2%E8%AF%95%E4%B8%80/</guid>
      <description>一、沟通
要有自信，即使在学校没咋参与啥项目，也要装的自己爱技术、爱交流，不要表现成看闲书的傻瓜。
二、动手实践
这个技术了解吗，了解；看过相关书吗，看过；动手实践了吗，没有。。。 一门技术，要有自己的见解，总结成自己的看法。
三、解决问题
项目中遇到的问题，解决思路，下一步要做什么。
再不折腾自己，一年后连个二十几人的小公司都进不去，连专科生都不如。。。</description>
    </item>
    
    <item>
      <title>前后端分离</title>
      <link>https://faded828xx.github.io/post/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/</link>
      <pubDate>Sun, 18 Jul 2021 22:53:19 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB/</guid>
      <description>这位大佬讲的很好，全篇高能
看了程序羊部署的ruoyi-vue前后端分离项目，去了解一下啥事前后端分离。虽然现在连SpringBoot都还不会。sad
前端开发与后端api进行交互时，往往需要后端服务器传来的数据，因此开发过程中往往过度依赖后端，尤其以jsp更为明显。若是不启动服务器，甚至连完整的html页面都看不到。而node.js的出现改变了这个现象，前端开发时可以通过部署node服务器来获取api数据，具体的操作现在还不太清楚。 当前后端分离后，前端项目和后端项目可以分别部署多台服务器，互不影响且降低访问压力。
我现在大概理解的开发模式有三种：
一、jsp模式 以ForestBlog项目为例
后端服务器需要同时管理页面的生成和api，由于浏览器不能识别jsp页面，因此服务器需要将jsp页面和api数据一起渲染成html页面后，再发送给客户端，当然此时客户端仍然可以调用服务器的api获得json数据，但大部分api在渲染页面时已经由服务器自己调用了。
二、前后端分离模式
这是不使用jsp，服务器可以直接发送静态页面给客户端，这时的页面是不完整的，浏览器还会向服务器发送api请求，服务器返回json数据后，浏览器会将其渲染到自己的页面中。
前端项目和后端项目分别部署到两台服务器上，浏览器发送页面请求时， 前端服务器首先返回不完整的html页面，浏览器再请求api数据，前端服务器将这些请求转发到后端服务器，再将后端返回到json数据传达给浏览器，浏览器自行渲染页面。
因此，在浏览器开发者模式，network-&amp;gt;XHR，大多数网站可以看到浏览器很多调用api的记录，而jsp项目只有很少的api记录，虽然页面中有很多数据库相关的动态数据，但这些不会显示在api记录中，因为浏览器接受到的页面是已经渲染好的，无需再发送api请求。
三、 浏览器渲染给客户端造成了压力，且首页等待时间更长，以及其他相关问题。因此出现了前端服务器渲染页面的模式，这一篇blog吧，还不是很了解，待填坑。 服务端渲染
可见，jsp模式开发后端过程中，与前端高度耦合，在controller层需要返回具体的jsp页面和model数据。而二三两种模式，只需要管理api和json数据。</description>
    </item>
    
    <item>
      <title>Ubuntu配置ssh</title>
      <link>https://faded828xx.github.io/post/ubuntu%E9%85%8D%E7%BD%AEssh/</link>
      <pubDate>Tue, 13 Jul 2021 14:54:07 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/ubuntu%E9%85%8D%E7%BD%AEssh/</guid>
      <description>用parallel desktop安装了ubuntu的虚拟机，官网下载镜像。iso 白嫖试用版，好像可以改系统时间来一直续。
虚拟机上网络改成wifi，因此和mac处于同一个局域网。mac上跑了tomcat服务器，ubuntu的tomcat能访问到。
了解到ssh可以实现通信，一开始连接mac向ubuntu和ubuntu向mac都是connection refused，应该是ssh 的client默认是系统自带的，而server需要下载，于是ubuntu下载：
下载： sudo apt install openssh-server 启动： sudo service ssh status（or start） mac终端连接： ssh faded828x@192.168.1.5 用transmit软件连接后能直接拖文件到ubuntu上，mac终端连接后可以直接在本地运行ubuntu终端命令。</description>
    </item>
    
    <item>
      <title>Tomcat部署Servlet</title>
      <link>https://faded828xx.github.io/post/tomcat%E9%83%A8%E7%BD%B2servlet/</link>
      <pubDate>Fri, 09 Jul 2021 19:54:52 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/tomcat%E9%83%A8%E7%BD%B2servlet/</guid>
      <description>Tomcat与JavaWeb开发技术详解，有了基础后，去年囫囵吞枣学的一些零零散散的知识总算明白了点。 Servlet是web服务器与web应用之间的接口。服务器部分由开源软件构成，包括网络编程和路由功能，Tomcat通过请求参数找到servlet接口的具体实现类；应用部分往往是项目代码，包含了具体的servlet实现类。这个servlet规范由oracle公司制定，服务器和应用开发者通过实现这一规范来适配对方。 Servlet无非就是网络编程加路由，根据socket拿到http报文，解析request参数，通过反射创建具体的servlet实现类，将响应报文写入socket的response中。而之所以说JSP仍是servlet，因为servlet实现类写入response时需要频繁增加html字符串，因此根据request参数将一些变量传入jsp页面会显得清晰，只需维护变量即可，仍是向socket中写入response。
IDEA中部署Tomcat： 注意Tomcat相关配置，servlet编译要添加tomcat中的jar库servlet-api（其他api好像也行），project structure - artifacts - output root 是编译文件结构。 代码参见github，目录如下： </description>
    </item>
    
    <item>
      <title>MySQL索引</title>
      <link>https://faded828xx.github.io/post/mysql%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Wed, 07 Jul 2021 16:09:36 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/mysql%E7%B4%A2%E5%BC%95/</guid>
      <description>暑假的学习计划开始啦， 目标还不是很明确， 先挑重要的学吧， MySQL之前学过一点简单的SQL语句， 知道一些索引、事务的名词，仅此而已。从图书馆借了大佬推荐的“MySQL是怎样运行的”， 开始记录。MacOS上下载MySQL之后会在系统设置界面出现图标可以开启服务器， 挺好玩的。
前面几章简单的提一下就行： MySQL程序分为服务器端和客户端，用不同的bin程序启动。table表等都存在服务器端， 客户端发出SQL命令， 服务器更新数据库后将结果返回。一般服务器会常开启，客户端随时接入连接或断开。navicat、IDEA内集成的数据库接口应该都是客户端。 字符编码和行格式不多说了。
（第六章前） 索引：B+树， 节点里是一页，封装了多条目录记录（内结点）或者用户记录（table中的行，叶子结点）。同时是二叉平衡搜索树，相关操作O(lgn)复杂度。 聚簇索引根据主键排序，节点里包含主键，用户记录都存在聚簇索引的叶结点中，创建表时自动创建该索引。 二级索引依据其他列值排序，因为可能出现相同的情况，因此结点中还包含主键来辅助排序，叶结点找到主键后，通过聚簇索引的B+树进行回表操作来得到完整的用户记录。 联合索引在二级索引的基础上添加多个列值用来优先级排序。 感觉这里没讲清楚，先通过筛选条件来建立索引，然后能快速查询。
一些SQL命令：
create table ... ( ... INDEX idx_name (col1) ); alter table ... add INDEX idx_name (col1); alter table ... drop INDEX idx_name; </description>
    </item>
    
    <item>
      <title>Leetcode773 752 BFS模版</title>
      <link>https://faded828xx.github.io/post/leetcode773-752-bfs%E6%A8%A1%E7%89%88/</link>
      <pubDate>Sat, 26 Jun 2021 19:34:48 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/leetcode773-752-bfs%E6%A8%A1%E7%89%88/</guid>
      <description>考试周了， 忙着复习， 贴两道算法题水水博客吧， 都是用的BFS模版。
Leetcode752
class OpenTheLock752 { public static int openLock(String[] deadends, String target) { int tar = Integer.parseInt(target); if (tar == 0) return 0; // target为0 无需变化 Set&amp;lt;Integer&amp;gt; dead = new HashSet&amp;lt;&amp;gt;(); Set&amp;lt;Integer&amp;gt; visited = new HashSet&amp;lt;&amp;gt;(); // 记忆化 防止死循环 for (String str : deadends) { // 感觉用整型好处理点 dead.add(Integer.parseInt(str)); } if (dead.contains(0)) // 0直接锁死 return -1; Deque&amp;lt;Integer&amp;gt; queue = new ArrayDeque&amp;lt;&amp;gt;(); // BFS queue.add(0); visited.add(0); int res = 0; while (!</description>
    </item>
    
    <item>
      <title>为什么使用ip地址</title>
      <link>https://faded828xx.github.io/post/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8ip%E5%9C%B0%E5%9D%80/</link>
      <pubDate>Sat, 12 Jun 2021 10:16:54 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8ip%E5%9C%B0%E5%9D%80/</guid>
      <description>最近计算机网络的书看了不少，一直困扰的问题是既然有了mac地址，虽然有48bit，可能相比IPv4的32bit计算量会剧增，但是相比IPv6的128bit就没压力了吧。因此既然mac地址是绑定在物理设备上的，为什么不用mac地址来直接在网络中寻址。今天在“从实践学习TCP/IP协议”找到了答案： mac地址理论上可易于在局域网中通信，但在路由式网络中，mac地址不能跨越路由接口。即使跨越，固定mac地址不能在地址空间上引入逻辑结构，无法表示具体的物理位置。比如说，两台设备A和B位于不同城市，因为公网IP往往是固定位置的，为新加入子网的设备分配IP，因此路由寻址是无论AB位于哪个城市，其子网的位置都是易寻的，再到子网中找到具体位置。但是用mac地址的话，因为是直接寻址，就会出现AB一会在这个城市，一会在另一个城市，中间的转发设备就要实时更新转发方向，这无疑是自找麻烦。mac地址不能反映物理地址，而公网IP应该在某段时间内是固定位置的。</description>
    </item>
    
    <item>
      <title>Go常量变量</title>
      <link>https://faded828xx.github.io/post/go%E5%B8%B8%E9%87%8F%E5%8F%98%E9%87%8F/</link>
      <pubDate>Wed, 02 Jun 2021 16:50:28 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/go%E5%B8%B8%E9%87%8F%E5%8F%98%E9%87%8F/</guid>
      <description>一个类型确定数字型常量所表示的值是不能溢出它的类型的表示范围的。 一个类型不确定数字型常量所表示的值是可以溢出它的默认类型的表示范围的。 当一个类型不确定数字常量值溢出它的默认类型的表示范围时，此数值不会被截断（亦即回绕）。 将一个非常量数字值转换为其它数字类型时，此非常量数字值可以溢出转化结果的类型。 在此转换中，当溢出发生时，转化结果为此非常量数字值的截断（亦即回绕）表示。
总结下来就是：类型确定常量不能截断，因此不能溢出或者一些奇怪的强转，而变量可以强转来截断。这里所说的常量和变量都是指等号右边，大部分与等号左边无关。贴几个例子：
// 三个类型不确定常量。 const n = 1 &amp;lt;&amp;lt; 64 // 默认类型为int const r = &#39;a&#39; + 0x7FFFFFFF // 默认类型为rune const x = 2e+308 // 默认类型为float64 const a uint8 = 256 // error: 256溢出uint8	与a是否const无关 const b = uint8(255) + uint8(1) // error: 256溢出uint8 与b是否const无关 const c = int8(-128) / int8(-1) // error: 128溢出int8 const a = -1.23 // 变量b的类型被推断为内置类型float64。 var b = a // error: 常量1.</description>
    </item>
    
    <item>
      <title>Leetcode1707与数组中元素的最大异或值</title>
      <link>https://faded828xx.github.io/post/leetcode1707%E4%B8%8E%E6%95%B0%E7%BB%84%E4%B8%AD%E5%85%83%E7%B4%A0%E7%9A%84%E6%9C%80%E5%A4%A7%E5%BC%82%E6%88%96%E5%80%BC/</link>
      <pubDate>Sun, 23 May 2021 23:33:59 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/leetcode1707%E4%B8%8E%E6%95%B0%E7%BB%84%E4%B8%AD%E5%85%83%E7%B4%A0%E7%9A%84%E6%9C%80%E5%A4%A7%E5%BC%82%E6%88%96%E5%80%BC/</guid>
      <description>这两个月的刷题还挺不错的，最近好多异或题。 力扣1707
package BitManipulation; import java.util.Arrays; import java.util.Comparator; //leetcode submit region begin(Prohibit modification and deletion) class MaximumXorWithAnElementFromArray1707 { // 先通过忽略不超过mi的方式来降维 具体还是看题解吧 主要是前缀树的思想 public static int[] maximizeXor(int[] nums, int[][] queries) { int len = queries.length; int[] res = new int[len]; Arrays.sort(nums); int[][] queryArr = new int[len][3]; for(int i=0; i&amp;lt;len; i++) { queryArr[i][0] = queries[i][0]; queryArr[i][1] = queries[i][1]; queryArr[i][2] = i; } Arrays.sort(queryArr, Comparator.comparingInt(ints -&amp;gt; ints[1])); int index = 0; Trie trie = new Trie(); for(int[] query : queryArr) { int x = query[0]; int m = query[1]; int id = query[2]; while(index&amp;lt;nums.</description>
    </item>
    
    <item>
      <title>Helloworld汇编</title>
      <link>https://faded828xx.github.io/post/helloworld%E6%B1%87%E7%BC%96/</link>
      <pubDate>Sun, 16 May 2021 22:56:13 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/helloworld%E6%B1%87%E7%BC%96/</guid>
      <description>汇编语言是下学期的课，CSAPP之前看了汇编，是x86的CPU，就没在m1上跑，今天试了一下，当然跑出来的肯定是arm汇编。 terminal指令（开发者命令行自带clang编译器）:
faded828x@promote ~ % cd desktop faded828x@promote desktop % vim test.c faded828x@promote desktop % clang test.c -S -o test.s faded828x@promote desktop % vim test.s faded828x@promote desktop % clang test.s -o test.o faded828x@promote desktop % ./test.o Hello World!% faded828x@promote desktop % test.s:
 .p2align 2 _main: ; @main .cfi_startproc ; %bb.0: stp x29, x30, [sp, #-16]! ; 16-byte Folded Spill mov x29, sp .cfi_def_cfa w29, 16 .cfi_offset w30, -8 .cfi_offset w29, -16 adrp x0, l_.</description>
    </item>
    
    <item>
      <title>豆瓣scrapy</title>
      <link>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3scrapy/</link>
      <pubDate>Thu, 13 May 2021 22:10:36 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3scrapy/</guid>
      <description>用scrapy实现先前的豆瓣爬虫，这里爬了top250，但是结果不是预期的顺序。
import scrapy from scrapy_demo.items import MovieItem class DoubanSpider(scrapy.Spider): name = &#39;douban&#39; allowed_domains = [&#39;douban.com&#39;] start_urls = [ &#39;https://movie.douban.com/top250&#39;, ] def parse(self, response): for i in range(0, 9): url = response.urljoin(&#39;?start=&#39; + str(25 * i)) yield scrapy.Request(url, callback=self.parse_dir_contents) def parse_dir_contents(self, response): # file = &#39;movie.html&#39; # 新建html页面并将爬取到的页面写入其中 # open(file, &#39;w&#39;).write(str(response.body.decode(&#39;utf-8&#39;))) # html = str(response.body.decode(&#39;utf-8&#39;)) # bs = bs4.BeautifulSoup(html, &#39;html.parser&#39;) # for movie in bs.find_all(&#39;div&#39;, class_=&#39;item&#39;): # item = MovieItem() # movie = str(movie) # 每部电影信息 # movielink = re.</description>
    </item>
    
    <item>
      <title>Leetcode1723完成所有工作的最短时间</title>
      <link>https://faded828xx.github.io/post/leetcode1723%E5%AE%8C%E6%88%90%E6%89%80%E6%9C%89%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%9C%80%E7%9F%AD%E6%97%B6%E9%97%B4/</link>
      <pubDate>Sat, 08 May 2021 14:23:16 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/leetcode1723%E5%AE%8C%E6%88%90%E6%89%80%E6%9C%89%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%9C%80%E7%9F%AD%E6%97%B6%E9%97%B4/</guid>
      <description>五一去苏州玩了，然后回了趟家。最近在读CSAPP，博客落了一段时间。GoLang啥时候能捡起来啊 手动/facepalm 这里贴一篇力扣的每日一题，是道困难题。
public static int res = Integer.MAX_VALUE; public static int minimumTimeRequired(int[] jobs, int k) { backtrack(jobs, 0, new int[k], 0); return res; } /** * @param jobs 工作量 * @param jobIndex 当前待分配工作索引 * @param jobTime 员工工作分配 * @param curMax 当前分配情况的res值 */ public static void backtrack(int[] jobs, int jobIndex, int[] jobTime, int curMax) { if (jobIndex == jobs.length) { res = Math.min(res, curMax); return; } // 无序分配 因此多个员工分配量都为0时 任选其一即可 boolean flag = true; for (int i = 0; i &amp;lt; jobTime.</description>
    </item>
    
    <item>
      <title>背包问题一</title>
      <link>https://faded828xx.github.io/post/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%80/</link>
      <pubDate>Sun, 25 Apr 2021 16:49:37 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E4%B8%80/</guid>
      <description>这几天力扣每日一题尽是动态规划，气死了，这里总结一下刷题中背包问题的基本解法。力扣链接
问题形式多种多样，但本质上就是对给定数组取有序或无序排列，又或者是这个排列的长度或数量，其解决思路主要考虑三个点：
 给定数组元素可取一次还是多次，即0-1背包还是完全背包，体现在遍历target时是倒叙还是正序 排列是有序还是无序，体现在外循环是nums数组（无序）还是target（有序） 具体问题：组合问题，true false（还没做到），最大最小问题  具体题目可以看这几天github上力扣项目的提交，都在DP分类里面。</description>
    </item>
    
    <item>
      <title>字符串模式匹配</title>
      <link>https://faded828xx.github.io/post/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/</link>
      <pubDate>Tue, 20 Apr 2021 19:14:55 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D/</guid>
      <description>字符串模式匹配问题再经典不过了，暴力和API调用就不再赘述了。之前学的是RK算法，用的是Hash，还看了Java的indexOf源码，今天正好力扣每日一题又刷到，来学习大名鼎鼎的KMP算法。直接上代码：
// KMP字符串匹配 利用前缀数组 当不匹配时 忽略已匹配字符中的相同前后缀 public int strStr(String haystack, String needle) { int lenS = haystack.length(); int lenN = needle.length(); if(lenN==0) return 0; int[] next = new int[lenN]; // needle的前缀数组 最长公共前后缀 for(int i=1, j=0; i&amp;lt;lenN; i++) { // 更新needle的前缀数组 while(j&amp;gt;0 &amp;amp;&amp;amp; needle.charAt(i)!=needle.charAt(j)) { // 若next[i]与next[j]不相等 则next[i]&amp;lt;=next[i-1] j = next[j-1]; } if(needle.charAt(i)==needle.charAt(j)) // next[i]与next[j]相等 则next[i]=next[i-1]+1 j++; next[i] = j; // i为索引 j为前缀的长度 } for(int i=0, j=0; i&amp;lt;lenS; i++) { // 开始匹配 i为haystack索引 j为needle中已匹配字符数 while(j&amp;gt;0 &amp;amp;&amp;amp; haystack.</description>
    </item>
    
    <item>
      <title>豆瓣小爬虫</title>
      <link>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3%E5%B0%8F%E7%88%AC%E8%99%AB/</link>
      <pubDate>Sun, 18 Apr 2021 14:29:53 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/%E8%B1%86%E7%93%A3%E5%B0%8F%E7%88%AC%E8%99%AB/</guid>
      <description>urllib爬取豆瓣网站
bs4解析html页面，获取有用信息并封装到数组中
sqlite3数据库，将数组存入其中
flask框架作为服务器，路由url地址到指定html页面，并从数据库取出变量动态写入页面
import re import sqlite3 import urllib.request import bs4 from flask import Flask, render_template import ssl ssl._create_default_https_context = ssl.SSLContext app = Flask(__name__) @app.route(&#39;/movie&#39;) def movie(): movies = saveDB() return render_template(&#39;movie.html&#39;, movies=movies) if __name__ == &#39;__main__&#39;: app.run() def getData(): # 返回25部电影数组 html = getHtml(&#39;https://movie.douban.com/top250&#39;) bs = bs4.BeautifulSoup(html, &#39;html.parser&#39;) datalist = [] # 存储25部电影 for item in bs.find_all(&#39;div&#39;, class_=&#39;item&#39;): data = [] item = str(item) # 每部电影信息 movielink = re.</description>
    </item>
    
    <item>
      <title>Hugo建站</title>
      <link>https://faded828xx.github.io/post/hugo%E5%BB%BA%E7%AB%99/</link>
      <pubDate>Fri, 16 Apr 2021 19:26:10 +0800</pubDate>
      
      <guid>https://faded828xx.github.io/post/hugo%E5%BB%BA%E7%AB%99/</guid>
      <description>Hugo是go开发的，用于为Github Pages生成静态网站，在本地编辑markdown文件后push到git仓库，username.github.io便会自动更新博客。Hugo据说相比其他工具比如Hexo等轻量化，且提供了许多主题。
hugo new site hugo_hugo.386_blog cd hugo_hugo.386_blog git clone https://gitlab.com/maxlefou/hugo.386 themes/hugo.386 hugo new post/First.md hugo server -t hugo.386 --buildDrafts hugo --theme=hugo.386 --baseUrl=&amp;quot;https://faded828xx.github.io/&amp;quot; --buildDrafts git init git add . git commit -m &amp;quot;first commit: hugo blog&amp;quot; git remote add origin https://github.com/faded828xx/faded828xx.github.io.git git push -u origin master </description>
    </item>
    
  </channel>
</rss>
